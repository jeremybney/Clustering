"""

"""

@options
    self.max_clusters = max_clusters # Number of clusters to return
    self.wght_lower = wght_lower     # Rel. weight of lower approxs
    self.wght_upper = wght_upper     # Rel. weight of upper approxs
    self.dist_threshold = None       # Threshold for cluster similarity
    self.p_param = p_param           # parameter for weighted distance
                                       centroid option below
    self.weighted_distance = wght    # Option (True) to use weighted
                                       distance centroid calculations
@notes
Distance threshold option:
    self.dist_threshold = 1.25 by default (entity assigned to all
    centroids within 25% of the optimal cluster distance)
    if self.dist_threshold <=1.0 conventional kmeans clusters returned
    The larger self.dist_threshold the more rough (entity overlap) will
    exist across all k clusters returned
Lower and Upper Approximation Weight options:
    SUM(wght_lower,wght_upper) must equal 1.0, else it will be set to
    defaults on execution
    wght_lower=0.75 by default
    wght_upper=0.25 by default
    The larger wght_lower is relative to wght_upper the more important
    cluster lower approximations will be and v.v
    
"""
##############################################################################
# Externals
import warnings
import time
import operator
import numpy as np
from copy import deepcopy


class RoughKMeans:

    def __init__(self,input_data,
                 max_clusters,
                 #wght_lower=0.75,
                 #wght_upper=0.25,
                 threshold=1.25,
                 p_param=1.0,
                 wght=False):

        # Rough clustering options
        self.normalize = False            # Option to Z-score normalize features
        self.max_clusters = max_clusters    # Number of clusters to return
        self.dist_threshold = threshold     # <=1.0 threshold for centroids indiscernibility
        self.tolerance = 1.0e-04            # Tolerance for stopping iterative clustering
        self.previous_error = 1.0e+32       # Back storage of centroid error
        self.wght_lower = wght_lower        # Rel. weight of lower approx for each cluster centroid
        self.wght_upper = wght_upper        # Rel. weight of upper approx to each cluster centroid
        self.p_param = p_param              # parameter for weighted distance centroid option
        self.weighted_distance = wght       # Option (True) to use alt. weighted distance centroid

        # Enforce wght_lower + wght_upper == 1.0
        if self.wght_lower + self.wght_upper > 1.0:
            self.wght_lower = 0.75
            self.wght_lower = 0.25
            warnings.warn("Upper + Lower Weights must == 1.0, Setting Values to Default")

        # Rough clustering internal vars
        self.data = input_data
        self.data_array = None
        self.feature_names = input_data.keys()
        self.data_length = len(self.data[self.feature_names[0]])

        # Rough clustering external vars
        self.keylist = None                 # Ordered list of keys
        self.tableau_lists = None           # List order of data keys for centroid arrays
        self.centroids = {}                 # Centroids for all returned clusters
        self.cluster_list = {}              # Internal listing of membership for all clusters
        self.distance = {}                  # Entity-cluster distances for all candidate clusters
        self.clusters = None                # upper and lower approx membership for all clusters
        self.d_weights = {}                 # Weight func. for entities if weighted_distance = True

        # Overhead
        self.timing = True                  # Timing print statements flag
        self.debug = False                  # Debug flag for entire class print statements
        self.debug_assign = False           # Debug flag assign_cluster_upper_lower_approximation()
        self.debug_dist = False             # Debug flag get_entity_centroid_distances()
        self.debug_update = False           # Debug flag update_centroids()
        self.small = 1.0e-04
        self.large = 1.0e+10

    def get_rough_clusters(self):

        """
        Run iterative clustering solver for rough k-means and return
        max_cluster rough clusters
        :return: self.centroids, self.assignments, self.upper_approximation,
        self.lower_approximation
        """

        # Transform data to nd-array for speed acceleration
        self.transform_data()

        # Get initial random entity clusters
        self.initialize_centroids()

        if self.dist_threshold <= 1.0:
            warnings.warn("Rough distance threshold set <= 1.0 and will produce conventional \
            k-means solution")

        # Iterate until centroids convergence
        ct = 0
        stop_flag = False
        while stop_flag is False:

            t1 = time.time()
            # Back-store centroids
            prev_centroids = deepcopy(self.centroids)

            # Get entity-cluster distances
            self.get_entity_centroid_distances()

            # Compute upper and lower approximations
            self.assign_cluster_upper_lower_approximation()

            # Update centroids with upper and lower approximations
            if self.weighted_distance is True:        # Run entity-centroid weighted distance update
                self.update_centroids_weighted_distance()
            else:   # Run standard rough k-means centroid update
                self.update_centroids()

            # Determine if convergence reached
            stop_flag = self.get_centroid_convergence(prev_centroids)

            t2 = time.time()
            iter_time = t2-t1
            print "Clustering Iteration", ct, " in: ", iter_time," secs"
            ct += 1

        return

    def transform_data(self):

        """
        Convert input data dictionary to float nd-array for
        accelerated clustering speed
        :var self.data
        :return: self.data_array
        """

        t1 = time.time()
        self.keylist = self.data.keys()
        self.tableau_lists = [self.data[key][:] for key in self.data]
        self.data_array = np.asfarray(self.tableau_lists).T

        # Normalize if requested
        if self.normalize is True:
            self.data_array -= np.mean(self.data_array, axis=0)
            tmp_std = np.std(self.data_array, axis=0)
            for i in range(len(self.data_array[0, :])):
                if tmp_std[i] >= 0.001:
                    self.data_array[:, i] /= tmp_std[i]

        if self.timing is True:
            t3 = time.time()
            print "transform_data Time",t3-t1
            print "shape",self.data_array.shape

    def initialize_centroids(self):

        """
        Randomly select [self.max_clusters] initial entities as
        centroids and assign to self.centroids
        :var self.max_clusters
        :var self.data
        :var self.data_array
        :var self.feature_names
        :return: self.centroids : current cluster centroids
        """

        t1 = time.time()

        # Select max cluster random entities from input and assign as
        # initial cluster centroids
        candidates = np.random.permutation(self.data_length)[0:self.max_clusters]

        if self.debug is True:
            print "Candidates",candidates,self.feature_names,self.data

        # self.centroids = {str(k): {v: self.data[v][candidates[k]] for v in self.feature_names} for
        #                  k in range(self.max_clusters)}

        self.centroids = {str(k): self.data_array[candidates[k], :] for k in
                          range(self.max_clusters)}

        if self.timing is True:
            print 'Max Clusters',self.max_clusters
            t3 = time.time()
            print "initialize_centroids Time",t3-t1

        return

    def get_centroid_convergence(self,previous_centroids):

        """
        Convergence test. Determine if centroids have changed, if so, return False, else True
        :arg previous_centroids : back stored values for last iterate centroids
        :var self.centroids
        :var self.feature_names
        :var self.tolerance
        :return boolean : centroid_error <= self.tolerance (True) else (false)
        """

        t1 = time.time()

        # centroid_error = np.sum([[abs(self.centroids[k][val] - previous_centroids[k][val])
        #                          for k in self.centroids] for val in self.feature_names])

        centroid_error = np.sum([np.linalg.norm(self.centroids[k] - previous_centroids[k])
                                 for k in self.centroids])

        if self.timing is True:
            t3 = time.time()
            print "get_centroid_convergence Time",t3-t1, " with error:",centroid_error

        if self.debug is True:
            print "Centroid change", centroid_error

        if centroid_error <= self.tolerance or np.abs(self.previous_error - centroid_error) \
                <= self.tolerance:
            return True
        else:
            self.previous_error = centroid_error.copy()
            return False

    def update_centroids_weighted_distance(self):

        """
        Update rough centroids for all candidate clusters given their
        upper/lower approximations set membership plus a weighted
        distance function based on distance of each entity to the given
        cluster centroid
        Cluster centroids updated/modified for three cases:
            if sets {lower approx} == {upper approx}, return
                conventional k-means cluster centroids
            elif set {lower approx] is empty and set {upper approx}
                is not empty, return upper-lower centroids
            else return weighted mean of lower approx centroids and
                upper-lower centroids
        :var self.data_array
        :var self.wght_lower
        :var self.wght_upper
        :var self.feature_names
        :var self.clusters
        :var self.d_weights
        :return: self.centroids : updated cluster centroids
        """

        t1 = time.time()

        for k in self.clusters:

            if len(self.clusters[k]["lower"]) == len(self.clusters[k]["upper"]) and \
                            len(self.clusters[k]["lower"]) != 0:
                # Get lower approximation vectors and distance weights
                weights = np.asarray([self.d_weights[k][str(l)] for l in self.clusters[k]["lower"]])
                weights /= np.sum(weights)
                self.centroids[str(k)] = \
                    np.sum([weights[m] * self.data_array[l,:]
                            for m,l in enumerate(self.clusters[k]["lower"])], axis=0)

            elif len(self.clusters[k]["lower"]) == 0 and len(self.clusters[k]["upper"]) != 0:
                # Get upper approximation vectors
                weights = np.asarray(
                    [self.d_weights[k][str(l)] for l in self.clusters[k]["upper"]])
                weights /= np.sum(weights)
                self.centroids[str(k)] = \
                    np.sum([weights[m] * self.data_array[l, :]
                            for m,l in enumerate(self.clusters[k]["upper"])], axis=0)

            else:
                # Get both upper-exclusive and lower approximation sets
                exclusive_set = \
                    list(set(self.clusters[k]["upper"]).difference(set(self.clusters[k]["lower"])))
                weights1 = np.asarray(
                    [self.d_weights[k][str(l)] * self.data_array[l, :]
                     for l in self.clusters[k]["lower"]])
                weights1 /= np.sum(weights1)
                weights2 = np.asarray(
                    [self.d_weights[k][str(l)] * self.data_array[l, :] for l in exclusive_set])
                weights2 /= np.sum(weights2)
                self.centroids[str(k)] = \
                    self.wght_lower * np.sum([weights1[m] * self.data_array[l, :]
                                              for m,l in enumerate(self.clusters[k]["lower"])], axis=0) \
                    + self.wght_upper * np.sum([weights2[m] * self.data_array[l, :]
                                                for m,l in enumerate(exclusive_set)], axis=0)

            if self.debug_update is True:
                print """###Cluster""", k, self.clusters[k]["lower"], self.clusters[k]["upper"]

        if self.timing is True:
            t3 = time.time()
            print "update_centroids Time", t3 - t1

        return

    def update_centroids(self):

        """
        Update rough centroids for all candidate clusters given their
        upper/lower approximations set membership
        Cluster centroids updated/modified for three cases:
            if sets {lower approx} == {upper approx}, return
                conventional k-means cluster centroids
            elif set {lower approx] is empty and set {upper approx}
                is not empty, return upper-lower centroids
            else return weighted mean of lower approx centroids and
                upper-lower centroids
        :var self.data_array
        :var self.wght_lower
        :var self.wght_upper
        :var self.feature_names
        :var self.clusters
        :return: self.centroids : updated cluster centroids
        """

        t1 = time.time()

        for k in self.clusters:

            if len(self.clusters[k]["lower"]) == len(self.clusters[k]["upper"]):
                # Get lower approximation vectors
                lower = self.data_array[self.clusters[k]["lower"], :]
                self.centroids[str(k)] = np.mean(lower,axis=0)

            elif len(self.clusters[k]["lower"]) == 0 and len(self.clusters[k]["upper"]) != 0:
                # Get upper approximation vectors
                upper = self.data_array[self.clusters[k]["upper"], :]
                self.centroids[str(k)] = np.mean(upper,axis=0)

            else:
                # Get both upper-exclusive and lower approximation sets
                # upper = self.data_array[self.clusters[k]["upper"], :]
                lower = self.data_array[self.clusters[k]["lower"], :]
                exclusive_set = \
                    list(set(self.clusters[k]["upper"]).difference(set(self.clusters[k]["lower"])))
                boundary = self.data_array[exclusive_set, :]
                self.centroids[str(k)] = \
                    self.wght_lower*np.mean(lower,axis=0) + self.wght_upper*np.mean(boundary,axis=0)

            if self.debug_update is True:
                print """###Cluster""", k, self.clusters[k]["lower"], self.clusters[k]["upper"]

        if self.timing is True:
            t3 = time.time()
            print "update_centroids Time", t3 - t1

        return

    def assign_cluster_upper_lower_approximation(self):

        """
        Compute entity-to-cluster optimal assignments +
        upper/lower approximations for all current clusters
        :var self.distance
        :var self.distance_threshold
        :var self.cluster_list
        :var self.max_clusters
        :var self.data_length
        :return: self.clusters[clusters]["upper"] : upper approx.
        :return: self.clusters[clusters]["lower"] : lower approx.
        """

        t1 = time.time()

        # Reset clusters and distance weights for each method call
        self.clusters = {str(q): {"upper": [], "lower": []} for q in range(self.max_clusters)}
        self.d_weights = {str(q): {} for q in range(self.max_clusters)}

        # Assign each entity to cluster upper/lower approximations as appropriate
        for k in range(0, self.data_length):
            v_clust = self.cluster_list[str(k)]     # Current entity nearest cluster

            # Compile all clusters for each entity that are within
            # self.threshold distance of best entity cluster
            T = {j: self.distance[str(k)][j] / np.max([self.distance[str(k)][v_clust], self.small])
                 for j in self.distance[str(k)] if
                 (self.distance[str(k)][j] / np.max([self.distance[str(k)][v_clust], self.small])
                  <= self.dist_threshold)
                 and (v_clust != j)}

            # Assign entity to lower and upper approximations of all clusters as needed
            if len(T.keys()) > 0:
                self.clusters[v_clust]["upper"].append(k)      # Assign entity to its nearest cluster upper approx.
                self.d_weights[v_clust][str(k)] = \
                    ((2 / np.pi) * np.arctan(-self.p_param * (self.distance[str(k)][v_clust]))) + 1
                for cluster_name in T:
                    self.clusters[cluster_name]["upper"].append(k)  # Assign entity to upper approx of near cluster
                    self.d_weights[cluster_name][str(k)] = \
                        ((2 / np.pi) * np.arctan(-self.p_param * (self.distance[str(k)][cluster_name]))) + 1
            else:
                self.clusters[v_clust]["upper"].append(k)      # Assign entity to its nearest cluster upper approx.
                self.clusters[v_clust]["lower"].append(k)      # Assign entity to its nearest cluster lower approx.
                self.d_weights[v_clust][str(k)] = \
                    ((2 / np.pi) * np.arctan(-self.p_param * (self.distance[str(k)][v_clust]))) + 1
            if self.debug_assign is True:
                print "Current Cluster", v_clust
                print "distance", self.distance[str(k)]
                print "T",T

        if self.timing is True:
            t3 = time.time()
            print "assign_cluster_upper_lower_approximation Time", t3 - t1

        return

    def get_entity_centroid_distances(self):

        """
        Compute entity-cluster distances and find nearest cluster
        for each entity and assign for all entities
        :var self.data_array : nd-array of all features for all entities
        :var self.centroids : nd-array of all cluster centroids
        :var self.max_clusters
        :return: self.distance : centroid-entity distance vectors
        :return self.cluster_list : best fit cluster-entity assignment
        """

        t1 = time.time()

        # Enumerate centroid distance vector for all entities and find nearest cluster and assign
        # distance1 = {}
        # for k in range(0,self.data_length):
        #     distance1[str(k)] = {str(j): np.linalg.norm([abs(self.data[val][k]-self.centroids[str(j)][val])
        #                                                      for val in self.feature_names])
        #                              for j in range(self.max_clusters)}
        #
        #     best_key = min(distance1[str(k)].iteritems(), key=operator.itemgetter(1))[0]
        #     self.cluster_list[str(k)] = best_key
        # t2 = time.time()

        tmp = []
        for l in range(0,self.max_clusters):
            tmp.append(np.linalg.norm(self.data_array - np.asarray(self.centroids[str(l)]),axis=1))

        for k in range(0,self.data_length):
            self.distance[str(k)] = {str(j): tmp[j][k] for j in range(self.max_clusters)}
            best_key = min(self.distance[str(k)].iteritems(), key=operator.itemgetter(1))[0]
            self.cluster_list[str(k)] = best_key

        if self.debug_dist is True:
            print "Cluster List",self.cluster_list
            print "Distances",self.distance

        # Determine self.dist_threshold based on percentile all entity-cluster distances
        # curr_dists = list(itertools.chain([self.distance[h][g] for h in self.distance for g in self.distance[h]]))
        # self.dist_threshold = np.percentile(curr_dists,50)

        if self.timing is True:
            t3 = time.time()
            print "get_entity_centroid_distances Time",t3-t1

        return

if __name__ == "__main__":

    """
    For class-level rough_clustering_tests see /rough_clustering_tests/rough_kmeans_tests.py
    """

    # Class Unit test
    data = {"test1": [1.0,1.0,2.1],"test2": [2.0,2.01,2.3],"test3": [3.,3.,3.1]}
    clstr = RoughKMeans(data,2,wght_lower=0.75,wght_upper=0.25,threshold=1.0,p_param=1.0,wght=True)
    clstr.get_rough_clusters()
    print "Final Rough k-means",clstr.cluster_list

##############################################################################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas as pd
import scipy as sc
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from scipy.spatial.distance import cdist

# Read processed customer stats data
df = pd.read_csv('/Users/jney/Desktop/NewClusters/May2018_v2.csv')
df = df.fillna(0)

median_deposits = df['deposits'].median(skipna=True)
df['deposits']=df.deposits.mask(df.deposits == 0,median_deposits)

df['percent_noncore'] = (100-(df['core_pairs_traded']/df['pairs_traded'])*100)
df['vol/deposits'] = (df['total_vol']/df['deposits'])
df['withdrawals/deposits'] = (df['withdrawals']/df['deposits'])
features_ = df.columns.tolist()

# Remove top 7 users, userids, and other columns from feature list, so we don't cluster them
df.drop(df.head(7).index, inplace=True)
del features_[features_.index('userid')]
del features_[features_.index('core_pairs_traded')]
del features_[features_.index('pairs_traded')]
del features_[features_.index('withdrawals')]
del features_[features_.index('deposits')]

print "Clustering ", df['userid'].count(), " users..."

# If any missing values are present, replace with median
#for name in features_:
#    df[name] = \
#        df[name].fillna(df[name].median())
        
#scale the values... or not
scaler = preprocessing.StandardScaler().fit(df[features_])
df_scaled = pd.DataFrame(scaler.transform(df[features_]), columns=features_) #only scale features_ list

# Convert dataframe to dictionary required by RoughKmeans only using the columns we want
cluster_dict = df_scaled[features_].to_dict(orient='list')
print cluster_dict.keys()
for value in cluster_dict.keys():
    print cluster_dict[value][0:40], np.max(cluster_dict[value]), np.min(cluster_dict[value])
    

#print "# of Keys", len(cluster_dict.keys())
#cluster_dict.pop("userid")                 # Remove customer_id from features to cluster
#try:
 #   cluster_dict.pop("No Transactions")         # Remove "no_transactions" since it is redundant with platform numbers
#except:
 #   pass

# Run rough Kmeans clustering
print "Running rough clustering on resulting feature space..."

# Set rough clustering parameters (SEE DOCS FOR DESCRIPTIONS)
num_clust = 5           # Number of clusters to return
wght_lower = 0.7        # Weight for lower approximation of rough clusters
wght_upper = 0.2        # Weight for upper approximation of rough clusters
threshold = 1.2       # Threshold by which centroids are considered indistinguishable (1.1 == 10% threshold)
p_param = 1.0
clstrk = RoughKMeans(cluster_dict,
                     num_clust,
                     #wght_lower,
                     #wght_upper,
                     threshold,
                     p_param,
                     wght=False)
clstrk.get_rough_clusters()

for i in range(clstrk.max_clusters):
    clt1 = str(i)
    print ""
    print "Cluster",clt1
    print "Total Unique Entities", len(clstrk.clusters[clt1]["lower"])
    print "Total All Entities",len(clstrk.clusters[clt1]["upper"])
    print "Total Shared Entities",len(set(clstrk.clusters[clt1]["upper"]).difference(set(clstrk.clusters[clt1]["lower"])))

#get centroids
rkm_centroid_may18 = pd.DataFrame.from_dict(clstrk.centroids, orient = 'index', columns = features_)

#find out how many users are shared between cluster 1 and cluster 2 using 'upper'
len(set(clstrk.clusters['1']['upper']) - set(clstrk.clusters['2']['upper']))
#find the userids for traders shared between cluster 1 and cluster 2
shared0_4 = df.userid.iloc[list(set(clstrk.clusters['0']['upper']).intersection(set(clstrk.clusters['4']['upper'])))]
shared2_3 = df.userid.iloc[list(set(clstrk.clusters['2']['upper']).intersection(set(clstrk.clusters['3']['upper'])))]
shared2_4 = df.userid.iloc[list(set(clstrk.clusters['2']['upper']).intersection(set(clstrk.clusters['4']['upper'])))]


set(clstrk.clusters['0']['upper']).intersection(set(clstrk.clusters['4']['upper']))

# kmeans determine k - elbow method for optimal number of clusters
distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k).fit(df_scaled)
    kmeanModel.fit(df_scaled)
    distortions.append(sum(np.min(cdist(df_scaled, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / df_scaled.shape[0])

# Plot the elbow
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()


################# MOST OF THIS BELOW DOES NOT WORK WELL

# Graph rough Kmeans results and save plots and centroids
unq_custm_ = []
plt.figure(figsize=(15, 8))
clr = ['r','b','g','k','m','y']

for i in range(clstrk.max_clusters):  # Capture results for each cluster
    clt1 = str(i)

    print "GROUP", clt1
    # Get the number of customers present in each rough cluster
    unq_custm_.append(len(clstrk.clusters[clt1]["lower"]))

    print "Totals Group, Lower, Upper", clt1, len(clstrk.clusters[clt1]["lower"]), len(
        clstrk.clusters[clt1]["upper"])

    # Write each cluster centroid to file and close
    file3 = open('Rough_Kmeans_GROUP_' + clt1 + '_.txt', 'w')
    for l, name in enumerate(clstrk.feature_names):
        file3.writelines(name + ',' + str(clstrk.centroids[clt1][l]) + '\n')
    file3.close()

    # Plot all groups on same stacked bar chart for comparison
    plt.title('Rough K-means Cluster Results',
              fontsize=14)
    if i == 0:
        # First sort centroid values by size, then store for labels
        arg_list = np.argsort(clstrk.centroids[clt1])
        first = np.asarray(clstrk.centroids[clt1])[arg_list]
        plt.bar(range(len(clstrk.feature_names)),first, color='r', align='center',
                label='Cluster ' + clt1 + ' : ' + str(unq_custm_[i])+' Unique Customers in Cluster')
        plt.ylabel('Screen Views (#) or Time (seconds)', fontsize=14)
        plt.xticks(range(len(clstrk.feature_names)),
                   np.asarray(clstrk.feature_names)[arg_list],
                   fontsize=14)
        plt.setp(plt.gca().xaxis.get_majorticklabels(),
                 rotation=90)
        plt.xlim([-1, len(clstrk.feature_names)])
        plt.tight_layout()
        plt.hold(True)
    else:
        plt.bar(range(len(clstrk.feature_names)),
                (np.asarray(clstrk.centroids[clt1])[arg_list]),
                color=clr[i],
                align='center',
                bottom=first,
                label='Cluster ' + clt1 + ' : ' + str(unq_custm_[i])+' Unique Customers in Cluster')
        first += np.asarray(clstrk.centroids[clt1])[arg_list]

plt.legend(loc='lower center')
plt.savefig('Rough_Kmeans_ALL_GROUPS_' + clt1 + '_hist.png')
plt.show()
